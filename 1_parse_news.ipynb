{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_create_news import try_load, save_s3\n",
    "from datetime import datetime, timedelta\n",
    "from dotenv import load_dotenv\n",
    "from minio import Minio\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "YANDEX_CLOUD_ACCESS_KEY = os.getenv(\"AWS_ACCESS_KEY_ID_2\")\n",
    "YANDEX_CLOUD_SECRET_KEY = os.getenv(\"AWS_SECRET_ACCESS_KEY_2\")\n",
    "\n",
    "API_KEY_NYT = os.getenv(\"API_KEY_NYT\")\n",
    "API_KEY_THE_GUARDIAN = os.getenv(\"API_KEY_THE_GUARDIAN\")\n",
    "\n",
    "BUCKET_NAME = 'graduate' # s3\n",
    "\n",
    "PATH = '/Users/dan/git_repo/graduate/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_s3 = Minio(\n",
    "    \"storage.yandexcloud.net\",\n",
    "    access_key=YANDEX_CLOUD_ACCESS_KEY,\n",
    "    secret_key=YANDEX_CLOUD_SECRET_KEY,\n",
    "    secure=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. New york times\n",
    "\n",
    "Сначала по API получим URL + Название статьи, затем спарсим текст"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выгружаем URL и Titile\n",
    "news_dict_nyt_url = try_load(file_path=PATH+'data/', file_name='news_dict_nyt_url.pkl', client_s3=client_s3)\n",
    "\n",
    "# Параметры запроса\n",
    "year_list = [2025,2025,2025,2024,2024,2024,2024,2024]\n",
    "month_list = [3,2,1,12,11,10,8,7]\n",
    "category = \"Politics\"\n",
    "\n",
    "for step in range(len(month_list)):\n",
    "\n",
    "    year = year_list[step]\n",
    "    month = month_list[step]\n",
    "    # Формируем URL запроса\n",
    "    url = f\"https://api.nytimes.com/svc/archive/v1/{year}/{month}.json?api-key={API_KEY_NYT}\"\n",
    "\n",
    "    # Делаем запрос к API\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Проверяем успешность запроса\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        \n",
    "        # Извлекаем все статьи\n",
    "        all_articles = data['response']['docs']\n",
    "\n",
    "        for i in range(len(all_articles)):\n",
    "            if all_articles[i]['news_desk'] == 'Politics':\n",
    "                titile = all_articles[i]['headline']['main']\n",
    "                url = all_articles[i]['web_url']\n",
    "\n",
    "                news_dict_nyt_url[titile] = url\n",
    "        \n",
    "    else:\n",
    "        print(f\"Ошибка запроса: {response.status_code}\")\n",
    "        print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "25\n",
      "50\n",
      "75\n",
      "100\n",
      "125\n",
      "150\n",
      "175\n",
      "200\n",
      "225\n",
      "250\n",
      "275\n",
      "300\n",
      "325\n",
      "350\n",
      "375\n",
      "400\n",
      "425\n",
      "450\n",
      "475\n",
      "500\n",
      "525\n",
      "550\n",
      "575\n",
      "600\n",
      "625\n",
      "650\n",
      "675\n",
      "700\n",
      "725\n",
      "750\n",
      "775\n",
      "800\n",
      "825\n",
      "850\n",
      "875\n",
      "900\n",
      "925\n",
      "950\n",
      "975\n",
      "1000\n",
      "1025\n",
      "1050\n",
      "1075\n",
      "1100\n",
      "1125\n",
      "1150\n",
      "1175\n",
      "1200\n",
      "1225\n",
      "1250\n",
      "1275\n",
      "1300\n",
      "1325\n",
      "1350\n",
      "1375\n",
      "1400\n",
      "1425\n",
      "1450\n",
      "1475\n",
      "1500\n",
      "1525\n",
      "1550\n",
      "1575\n",
      "1600\n",
      "1625\n",
      "1650\n",
      "1675\n",
      "1700\n",
      "1725\n",
      "1750\n",
      "1775\n",
      "1800\n",
      "1825\n",
      "1850\n",
      "1875\n",
      "1900\n",
      "1925\n",
      "1950\n",
      "1975\n",
      "2000\n",
      "2025\n",
      "2050\n",
      "2075\n",
      "2100\n",
      "2125\n",
      "2150\n",
      "2175\n",
      "2200\n",
      "2225\n",
      "2250\n",
      "2275\n",
      "2300\n",
      "\n",
      "Ошибка при запросе: 403, https://www.nytimes.com/2024/11/05/us/politics/election-threats-russia.html\n"
     ]
    }
   ],
   "source": [
    "# Спарсим сами статьи\n",
    "papers_nyt = try_load(file_path=PATH+'data/', file_name='papers_nyt.pkl', client_s3=client_s3)\n",
    "\n",
    "news_dict_keys = list(news_dict_nyt_url.keys())\n",
    "\n",
    "# Уберем лишнее\n",
    "pattern = re.compile(r'^(Advertisement|Supported by|By \\w+ \\w+|Reporting from \\w+|Fact Check|Trump Administration)$')\n",
    "\n",
    "# Создаем сессию\n",
    "session = requests.Session()\n",
    "\n",
    "\n",
    "# Обновляем заголовки сессии\n",
    "session.headers.update({\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 11.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36 Edg/112.0.1722.64',\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
    "    'Accept-Language': 'en-US,en;q=0.9',\n",
    "    'Accept-Encoding': 'gzip, deflate, br',\n",
    "    'Connection': 'keep-alive',\n",
    "    'Upgrade-Insecure-Requests': '1',\n",
    "    'Cache-Control': 'max-age=0',\n",
    "    'TE': 'Trailers',\n",
    "    'Dnt': '1',  # Do Not Track\n",
    "    'Sec-Fetch-Dest': 'document',\n",
    "    'Sec-Fetch-Mode': 'navigate',\n",
    "    'Sec-Fetch-Site': 'none',\n",
    "    'Sec-Fetch-User': '?1',\n",
    "    'Pragma': 'no-cache'\n",
    "})\n",
    "\n",
    "for i, title in enumerate(news_dict_keys[:2600]):\n",
    "    if i%25==0:\n",
    "        print(i)\n",
    "        with open('papers_nyt.pkl', 'wb') as file:\n",
    "            pickle.dump(papers_nyt, file)\n",
    "\n",
    "    url = news_dict_nyt_url[title]\n",
    "\n",
    "    if title not in papers_nyt.keys():\n",
    "        time.sleep(1)\n",
    "        # Делаем GET-запрос\n",
    "        response = session.get(url)\n",
    "\n",
    "        # Проверяем, что запрос успешный\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            # Попробуем найти основной текст статьи\n",
    "            # Вариант 1: ищем по <article>\n",
    "            article_body = soup.find('article')\n",
    "\n",
    "            # Вариант 2: ищем по div с более общими классами\n",
    "            if not article_body:\n",
    "                article_body = soup.find('div', {'class': 'css-1v6mj2p'})\n",
    "\n",
    "            # Если не нашли, пробуем другие элементы\n",
    "            if not article_body:\n",
    "                article_body = soup.find('section')\n",
    "\n",
    "            if article_body:\n",
    "                paragraphs = article_body.find_all('p')\n",
    "\n",
    "                full_text = '\\n'.join(p.get_text() for p in paragraphs)\n",
    "\n",
    "                full_text = \" \".join(line for line in full_text.splitlines() if not pattern.match(line))\n",
    "                full_text = full_text.split('We are having trouble retrieving the article content.')[0]\n",
    "\n",
    "                papers_nyt[title] = full_text\n",
    "\n",
    "            else:\n",
    "                print(\"Не удалось найти текст статьи на странице.\")\n",
    "        else:\n",
    "            print()\n",
    "            print(f\"Ошибка при запросе: {response.status_code}, {url}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PATH+'data/papers_nyt.pkl', 'wb') as file:\n",
    "    pickle.dump(papers_nyt, file)\n",
    "\n",
    "save_s3(pickle_data=papers_nyt, object_key='papers_nyt.pkl', client_s3=client_s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. The Guardian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Удаление HTML-тегов\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    clean_text = soup.get_text(separator=\" \", strip=True)\n",
    "    \n",
    "    # Удаление URL-адресов (http/https)\n",
    "    clean_text = re.sub(r'https?://\\S+|www\\.\\S+', '', clean_text)\n",
    "    \n",
    "    # Удаление лишних пробелов\n",
    "    clean_text = re.sub(r'\\s+', ' ', clean_text).strip()\n",
    "    \n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5200"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Загрузим оригинальные новости\n",
    "paper_dict = try_load(file_path=PATH+'data/', file_name='the_guardian_politic_CLEAN_news_09022025.pkl', client_s3=client_s3)\n",
    "\n",
    "query = \"politics\"  \n",
    "from_date = (datetime.now() - timedelta(days=60)).strftime('%Y-%m-%d')\n",
    "to_date = (datetime.now() - timedelta(days=30)).strftime('%Y-%m-%d')\n",
    "page_size = 200  \n",
    "\n",
    "# Формируем URL запроса\n",
    "url = f\"https://content.guardianapis.com/search?q={query}&from-date={from_date}&to-date={to_date}&page-size={page_size}&show-fields=body,headline,trailText,byline&show-tags=keyword&api-key={API_KEY_THE_GUARDIAN}\"\n",
    "\n",
    "# Делаем запрос к API\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    articles = data['response']['results']\n",
    "    \n",
    "    for article in articles:\n",
    "        # Проверяем, что статья относится к политике\n",
    "        if any(tag['webTitle'] == 'Politics' for tag in article.get('tags', [])):\n",
    "            \n",
    "            url = article['id']\n",
    "            titile = article['fields']['headline']\n",
    "            paper = clean_text(article['fields']['body'])\n",
    "\n",
    "            paper_dict[url] = [titile, paper]\n",
    "    \n",
    "else:\n",
    "    print(f\"Ошибка запроса: {response.status_code}\")\n",
    "    print(response.text)\n",
    "\n",
    "len(paper_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PATH+'data/the_guardian_politic_CLEAN_news_09022025.pkl', 'wb') as file:\n",
    "    pickle.dump(paper_dict, file)\n",
    "\n",
    "save_s3(pickle_data=paper_dict, object_key='the_guardian_politic_CLEAN_news_09022025.pkl', client_s3=client_s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graduate_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
