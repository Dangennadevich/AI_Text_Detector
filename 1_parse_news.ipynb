{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ноутбук с парсингом новостных статей\n",
    "\n",
    "* 0. Инициализация\n",
    "* 1. New york times\n",
    "* 2. The Guardian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_create_news import try_load, save_s3\n",
    "from datetime import datetime, timedelta\n",
    "from dotenv import load_dotenv\n",
    "from minio import Minio\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Инициализация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "YANDEX_CLOUD_ACCESS_KEY = os.getenv(\"AWS_ACCESS_KEY_ID_2\")\n",
    "YANDEX_CLOUD_SECRET_KEY = os.getenv(\"AWS_SECRET_ACCESS_KEY_2\")\n",
    "\n",
    "API_KEY_NYT = os.getenv(\"API_KEY_NYT\")\n",
    "API_KEY_THE_GUARDIAN = os.getenv(\"API_KEY_THE_GUARDIAN\")\n",
    "\n",
    "BUCKET_NAME = 'graduate' # s3\n",
    "\n",
    "PATH = '/Users/dan/git_repo/graduate/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_s3 = Minio(\n",
    "    \"storage.yandexcloud.net\",\n",
    "    access_key=YANDEX_CLOUD_ACCESS_KEY,\n",
    "    secret_key=YANDEX_CLOUD_SECRET_KEY,\n",
    "    secure=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. New york times\n",
    "\n",
    "Сначала по API получим URL + Название статьи, затем спарсим текст"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выгружаем URL и Titile\n",
    "# news_dict_nyt_url = dict()\n",
    "news_dict_nyt_url = try_load(file_path=PATH+'data/', file_name='news_dict_nyt_url.pkl', client_s3=client_s3)\n",
    "\n",
    "# Параметры запроса\n",
    "year_list = [2025,2025,2025,2024,2024,2024,2024,2024]\n",
    "month_list = [3,2,1,12,11,10,8,7]\n",
    "category = \"Politics\"\n",
    "\n",
    "for step in range(len(month_list)):\n",
    "\n",
    "    year = year_list[step]\n",
    "    month = month_list[step]\n",
    "    # Формируем URL запроса\n",
    "    url = f\"https://api.nytimes.com/svc/archive/v1/{year}/{month}.json?api-key={API_KEY_NYT}\"\n",
    "\n",
    "    # Запрос к API NYT\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        \n",
    "        # Извлекаем все статьи\n",
    "        all_articles = data['response']['docs']\n",
    "\n",
    "        for i in range(len(all_articles)):\n",
    "            if all_articles[i]['news_desk'] == 'Politics':\n",
    "                titile = all_articles[i]['headline']['main']\n",
    "                url = all_articles[i]['web_url']\n",
    "\n",
    "                news_dict_nyt_url[titile] = url\n",
    "        \n",
    "    else:\n",
    "        print(f\"Ошибка запроса: {response.status_code}\")\n",
    "        print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Спарсим сами статьи\n",
    "papers_nyt = try_load(file_path=PATH+'data/', file_name='papers_nyt.pkl', client_s3=client_s3)\n",
    "\n",
    "news_dict_keys = list(news_dict_nyt_url.keys())\n",
    "\n",
    "# налету уберем блоки, не относящиеся к статье\n",
    "pattern = re.compile(r'^(Advertisement|Supported by|By \\w+ \\w+|Reporting from \\w+|Fact Check|Trump Administration)$')\n",
    "\n",
    "# Создаем сессию\n",
    "session = requests.Session()\n",
    "\n",
    "# Обновляем заголовки сессии\n",
    "session.headers.update({\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 11.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36 Edg/112.0.1722.64',\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n",
    "    'Accept-Language': 'en-US,en;q=0.9',\n",
    "    'Accept-Encoding': 'gzip, deflate, br',\n",
    "    'Connection': 'keep-alive',\n",
    "    'Upgrade-Insecure-Requests': '1',\n",
    "    'Cache-Control': 'max-age=0',\n",
    "    'TE': 'Trailers',\n",
    "    'Dnt': '1', \n",
    "    'Sec-Fetch-Dest': 'document',\n",
    "    'Sec-Fetch-Mode': 'navigate',\n",
    "    'Sec-Fetch-Site': 'none',\n",
    "    'Sec-Fetch-User': '?1',\n",
    "    'Pragma': 'no-cache'\n",
    "})\n",
    "\n",
    "for i, title in enumerate(news_dict_keys):\n",
    "    if i%25==0:\n",
    "        print(i)\n",
    "        with open('papers_nyt.pkl', 'wb') as file:\n",
    "            pickle.dump(papers_nyt, file)\n",
    "\n",
    "    url = news_dict_nyt_url[title]\n",
    "\n",
    "    if title not in papers_nyt.keys():\n",
    "        time.sleep(1)\n",
    "        # GET-запрос\n",
    "        response = session.get(url)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            # Попробуем найти основной текст статьи\n",
    "            # Вариант 1: ищем по <article>\n",
    "            article_body = soup.find('article')\n",
    "\n",
    "            # Вариант 2: ищем по div с более общими классами\n",
    "            if not article_body:\n",
    "                article_body = soup.find('div', {'class': 'css-1v6mj2p'})\n",
    "\n",
    "            # Если не нашли, пробуем другие элементы\n",
    "            if not article_body:\n",
    "                article_body = soup.find('section')\n",
    "\n",
    "            if article_body:\n",
    "                paragraphs = article_body.find_all('p')\n",
    "\n",
    "                full_text = '\\n'.join(p.get_text() for p in paragraphs)\n",
    "                \n",
    "                # Небольшая предобработка\n",
    "                full_text = \" \".join(line for line in full_text.splitlines() if not pattern.match(line))\n",
    "                full_text = full_text.split('We are having trouble retrieving the article content.')[0]\n",
    "\n",
    "                papers_nyt[title] = full_text\n",
    "\n",
    "            else:\n",
    "                print(\"Не удалось найти текст статьи на странице.\")\n",
    "        else:\n",
    "            print()\n",
    "            print(f\"Ошибка при запросе: {response.status_code}, {url}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохраним спаршенные данные\n",
    "with open(PATH+'data/papers_nyt.pkl', 'wb') as file:\n",
    "    pickle.dump(papers_nyt, file)\n",
    "\n",
    "save_s3(pickle_data=papers_nyt, object_key='papers_nyt.pkl', client_s3=client_s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. The Guardian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5200"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Загрузим оригинальные новости\n",
    "paper_dict = try_load(file_path=PATH+'data/', file_name='the_guardian_politic_CLEAN_news_09022025.pkl', client_s3=client_s3)\n",
    "\n",
    "query = \"politics\"  \n",
    "from_date = (datetime.now() - timedelta(days=60)).strftime('%Y-%m-%d')\n",
    "to_date = (datetime.now() - timedelta(days=30)).strftime('%Y-%m-%d')\n",
    "page_size = 200  \n",
    "\n",
    "# Формируем URL запроса\n",
    "url = f\"https://content.guardianapis.com/search?q={query}&from-date={from_date}&to-date={to_date}&page-size={page_size}&show-fields=body,headline,trailText,byline&show-tags=keyword&api-key={API_KEY_THE_GUARDIAN}\"\n",
    "\n",
    "# Делаем запрос к API\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    articles = data['response']['results']\n",
    "    \n",
    "    for article in articles:\n",
    "        # Проверяем, что статья относится к политике\n",
    "        if any(tag['webTitle'] == 'Politics' for tag in article.get('tags', [])):\n",
    "            \n",
    "            url = article['id']\n",
    "            titile = article['fields']['headline']\n",
    "            paper = article['fields']['body']\n",
    "\n",
    "            paper_dict[url] = [titile, paper]\n",
    "    \n",
    "else:\n",
    "    print(f\"Ошибка запроса: {response.status_code}\")\n",
    "    print(response.text)\n",
    "\n",
    "len(paper_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохраним спаршенные данные\n",
    "with open(PATH+'data/the_guardian_politic_CLEAN_news_09022025.pkl', 'wb') as file:\n",
    "    pickle.dump(paper_dict, file)\n",
    "\n",
    "save_s3(pickle_data=paper_dict, object_key='the_guardian_politic_CLEAN_news_09022025.pkl', client_s3=client_s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graduate_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
